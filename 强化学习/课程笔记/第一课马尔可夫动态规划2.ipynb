{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward ：及时回报\n",
    "* #### 这是强化学习的一个非常独特的点\n",
    "* #### positive reward 如果这个数是正数就代表对这个行为是鼓励，奖励的\n",
    "* #### negative reward 如果这个数字是负数代表对这个行为是就惩罚的\n",
    "* #### 如果是 Reword 是 0 就是没有惩罚，没有惩罚就是奖励 \n",
    "* #### 也可以反过来，正数代表奖励，负数代表惩罚\n",
    "![2及时回报](./images/2及时回报.png)\n",
    "\n",
    "##### 如果进入[边界/boundary]就 $ $r_{\\text {bound }}=-1$ $\n",
    "##### 如果进入[/forbidden]就 $ $r_{\\text {forbid}}=-1$ $\n",
    "##### 如果进入[/target]就 $ $r_{\\text {target}}=+1$ $\n",
    "##### 如果进入其他的区域就  $ $r_{\\text {target}}=0$ $\n",
    "\n",
    "#### Reword其实是human-machine interface/人机接口，我们通过设计Reword来似程序实现目标\n",
    "#### 回报矩阵\n",
    "![及时回报矩阵](./images/2及时回报矩阵.png)\n",
    "#### 这个只能表示精确状态，不能表示不确定状态，这时候就要用条件概率\n",
    "* #### $p\\left(r=-1 \\mid s_1, a_1\\right)=1$ and $p\\left(r \\neq-1 \\mid s_1, a_1\\right)=0$\n",
    "* ##### 上面这个公式的意思的，$\\{在s_1状态下,动作是a_1的时候得到奖励函数r=-1的概率是1，得r\\neq-1的概率是0\\}$\n",
    "* ##### Reword及时奖励依赖的是当前的状态，而不是下一个状态\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trajectory:轨迹\n",
    "* #### Trajectory是一段状态动作序列，没有对长度的限制，R是一段Trajectory\n",
    "![](./images//2轨迹1.png)\n",
    "* #### 上面这个公式的意思的：在状态s_1的情况下执行动作a_1获得了奖励是r=0 这样持续下去，总计奖励Return的和是:0+0+0+1=1\n",
    "![](./images/2轨迹2.png)\n",
    "* #### 不同的轨迹Return出的总奖励不同，这一副图的轨迹奖励就是 0-1+0+1=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discounted return:打折后的回报\n",
    "![](./images/2打折后的回报.png)\n",
    "#### 上面这张图里，当state到达target后如果继续重复动作停留就会无限加+1加大大奖励return的综合会无限增加而变得发散掉，这时候我们就要加上discount rate/贴现率\n",
    "![](./images/2贴现率.png)\n",
    "\n",
    "####  以下是上面公式计算过程的python证明\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.9970309268517354e-06, 3.997030926852713e-06, 3.997030926852713e-06)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "gamma = np.random.uniform(0,1)\n",
    "n = 10\n",
    "\n",
    "gammas = np.zeros(n)\n",
    "gammas_sum = np.zeros(n)\n",
    "\n",
    "point = 3\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    if i < point:\n",
    "        gammas[i] = gamma**i*0\n",
    "    else:\n",
    "        gammas[i] = gamma**i*1\n",
    "    gammas_sum[i] = gamma ** i\n",
    "    \n",
    "gammas.sum(),gamma**point*(1/(1-gamma)),(gamma**point*gammas_sum).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episode:回合\n",
    "#### 它是智能体从开始执行任务，根据每个时刻的状态和对应的策略，依次选取一系列动作，直至任务终止的一个完整过程，就是一个episode。\n",
    "#### terminal states:终止状态\n",
    "![](./images/2回合.png)\n",
    "* #### 有Episode的通常都是有限步的任务，称为episode tasks\n",
    "* #### 有些任务是没有终止状态的，通常会一直持续下去，称为 countinuing tasks,通常现实里是没有这种任务的，但是时间比较长的话，我们会近似的认为就是这种任务\n",
    "* #### 有两种操作方法：\n",
    "* ##### option 1 : 把到达目标的[target state]的状态当成特殊[absorbing state/吸收状态]，就是在设置状态转移概率的时候，如果当前的状态是[target state]不论采取什么动作都会回到这个状态，或者修改它的动作空间[action space]，让他的动作只有这一个就是回到[target state]，并且把所有的reword全部改为0\n",
    "* ##### option 2 : 把到达目标的[target state]的状态当成普通策略，如果策略不好的话还可能跳出来。相对来说学习的时候会耗费更多的搜索，但是相对来说会更加的一般化普适。\n",
    "* #### 本课程用的是 option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/2MDP的要素.png)\n",
    "##### 马尔科夫动态规划的关键有哪些\n",
    "* ##### 集合[Set]\n",
    "* * ##### 状态[state]：状态的集合 $ S $\n",
    "* * ##### 动作[action]：动作的集合 $ A(a),s \\in S$\n",
    "* * ##### 回报[reword]：回报的集合 $ R(s,a) $\n",
    "* ##### 概率分布[Probability distribution]\n",
    "* * ##### 状态转移概率[State trainsition probability]:$ 在状态s情况下，执行策略a，跳到s^{'}的概率 p(s^{'}|s,a) $\n",
    "* * ##### 回报概率[Reword probability]:$ 在状态s情况下，执行策略a，得到回报的概率 p(r|s,a) $\n",
    "* ##### 策略[Policy]: $ 在状态s，采取动作 a 的概率 \\pi(a|s) $\n",
    "* ##### 马尔科夫性质[Markov property]:\n",
    "* * ##### 当前状态和历史无关,当前状态转移的概率和回报的概率都与历史无关\n",
    "\n",
    "\n",
    "\n",
    "#### 马尔科夫过程和马尔科夫动态规划的区别在于，如果我的策略[policy]是确定的,与整个系统融为一体的就是马尔科夫过程，如果是不确定的，就是马尔科夫动态规划"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XGBoostAndScikitOPT_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
