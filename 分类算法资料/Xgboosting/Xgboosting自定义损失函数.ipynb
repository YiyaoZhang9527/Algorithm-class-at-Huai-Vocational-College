{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd95b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# 加载数据集\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 转换数据格式为DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # 二分类逻辑回归\n",
    "#     'eval_metric': 'error',  # 评估指标为错误率\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_rounds = 100  # 迭代次数\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred_binary = [1 if p > 0.5 else 0 for p in y_pred]  # 将概率转换为类别\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcaa20a",
   "metadata": {},
   "source": [
    "## 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b0da979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9035087719298246\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# 加载数据集\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 转换数据格式为DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "def custom_loss(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "\n",
    "    preds = 1.0 / (1.0 + np.exp(-preds))\n",
    "\n",
    "    def binary_cross_entropy_gradient(y_pred, y_true):\n",
    "        eps = 1e-15  # 避免除零错误的常数\n",
    "\n",
    "        # 预测概率取值范围限制在 [eps, 1-eps] 内\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "        # 二分类交叉熵损失函数的一阶导数（梯度）\n",
    "        gradient = - y_true / y_pred + (1 - y_true) / (1 - y_pred)\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def binary_cross_entropy_hessian(y_pred, y_true):\n",
    "        eps = 1e-15  # 避免除零错误的常数\n",
    "\n",
    "        # 预测概率取值范围限制在 [eps, 1-eps] 内\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "        # 二分类交叉熵损失函数的二阶导数（海森矩阵）\n",
    "        hessian = y_true / (y_pred ** 2) + (1 - y_true) / ((1 - y_pred) ** 2)\n",
    "\n",
    "        return hessian\n",
    "\n",
    "    gradient = binary_cross_entropy_gradient(preds, labels)\n",
    "    hessian = binary_cross_entropy_hessian(preds, labels)\n",
    "\n",
    "    # 防止梯度爆炸做剪裁\n",
    "    l2 = np.linalg.norm(gradient)\n",
    "    max_norm = 0.43\n",
    "    if l2>max_norm:\n",
    "        gradient = gradient*(max_norm/l2)\n",
    "    l2 = np.linalg.norm(hessian)\n",
    "    if l2>max_norm:\n",
    "        hessian = hessian*(max_norm/l2)\n",
    "\n",
    "    return gradient, hessian\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'eval_metric': 'error',  # 评估指标为错误率\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "num_rounds = 100  # 迭代次数\n",
    "model = xgb.train(params, dtrain, num_rounds,obj=custom_loss)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred_binary = [1 if p > 0.1 else 0 for p in y_pred]  # 将概率转换为类别\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cf040f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "def maxRecall(preds,dtrain): #preds是结果（概率值），dtrain是个带label的DMatrix\n",
    "    labels=dtrain.get_label() #提取label\n",
    "    preds=1-preds\n",
    "    precision,recall,threshold=precision_recall_curve(labels,preds,pos_label=0)\n",
    "    pr=pd.DataFrame({'precision':precision,'recall':recall})\n",
    "    return 'Max Recall:',pr[pr.precision>=0.97].recall.max()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    " \n",
    "gamma = 0\n",
    "alpha = 0.5\n",
    " \n",
    "\n",
    "# y指的是true label\n",
    "# p指的是预测出来的概率值（是经过sigmoid转换之后  p = 1.0 / (1.0 + np.exp(-p))\n",
    "# gamma指的是local foss的参数\n",
    "# alpha指的是用来处理非平衡的参数, label为1的样本的损失是label为0的多少倍\n",
    "def logistic_obj(p, dtrain):\n",
    "    \n",
    "    y = dtrain.get_label()\n",
    "    p = 1.0 / (1.0 + np.exp(-p))\n",
    "    \n",
    "    grad = p * (1 - p) * (alpha * gamma * y * (1 - p) ** gamma * np.log(p) / (1 - p) - alpha * y * (\n",
    "                1 - p) ** gamma / p - gamma * p ** gamma * (1 - alpha) * (1 - y) * np.log(1 - p) / p + p ** gamma * (\n",
    "                                      1 - alpha) * (1 - y) / (1 - p))\n",
    "    hess = p * (1 - p) * (p * (1 - p) * (\n",
    "                -alpha * gamma ** 2 * y * (1 - p) ** gamma * np.log(p) / (1 - p) ** 2 + alpha * gamma * y * (\n",
    "                    1 - p) ** gamma * np.log(p) / (1 - p) ** 2 + 2 * alpha * gamma * y * (1 - p) ** gamma / (\n",
    "                            p * (1 - p)) + alpha * y * (1 - p) ** gamma / p ** 2 - gamma ** 2 * p ** gamma * (\n",
    "                            1 - alpha) * (1 - y) * np.log(1 - p) / p ** 2 + 2 * gamma * p ** gamma * (1 - alpha) * (\n",
    "                            1 - y) / (p * (1 - p)) + gamma * p ** gamma * (1 - alpha) * (1 - y) * np.log(\n",
    "            1 - p) / p ** 2 + p ** gamma * (1 - alpha) * (1 - y) / (1 - p) ** 2) - p * (\n",
    "                                      alpha * gamma * y * (1 - p) ** gamma * np.log(p) / (1 - p) - alpha * y * (\n",
    "                                          1 - p) ** gamma / p - gamma * p ** gamma * (1 - alpha) * (1 - y) * np.log(\n",
    "                                  1 - p) / p + p ** gamma * (1 - alpha) * (1 - y) / (1 - p)) + (1 - p) * (\n",
    "                                      alpha * gamma * y * (1 - p) ** gamma * np.log(p) / (1 - p) - alpha * y * (\n",
    "                                          1 - p) ** gamma / p - gamma * p ** gamma * (1 - alpha) * (1 - y) * np.log(\n",
    "                                  1 - p) / p + p ** gamma * (1 - alpha) * (1 - y) / (1 - p)))\n",
    "    \n",
    "    return grad, hess\n",
    " \n",
    "\n",
    "# dtrain = xgb.DMatrix('../demo_data/agaricus.txt.train')\n",
    "# dtest = xgb.DMatrix('../demo_data/agaricus.txt.test')\n",
    " \n",
    "# print(\"---------no focal loss-----------\")\n",
    " \n",
    "# watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    " \n",
    "# params = {'max_depth': 2, 'eta': 0.01, 'silent': 1, 'eval_metric': 'auc', \"objective\": \"binary:logistic\"}\n",
    "# xgb.train(params=params, dtrain=dtrain, num_boost_round=3, early_stopping_rounds=50,\n",
    "#           evals=[(dtrain, 'train'), (dtest, 'test')], verbose_eval=1)\n",
    " \n",
    "# print(\"---------focal loss-----------\")\n",
    "# params = {'max_depth': 2, 'eta': 0.01, 'silent': 1, 'eval_metric': 'auc', \"objective\": \"binary:logitraw\"}\n",
    "# xgb.train(params=params, dtrain=dtrain, num_boost_round=3, early_stopping_rounds=50,\n",
    "#           evals=[(dtrain, 'train'), (dtest, 'test')], verbose_eval=1, obj=logistic_obj)\n",
    "\n",
    "\n",
    "num_rounds = 100  # 迭代次数\n",
    "model = xgb.train(params, dtrain, num_rounds,custom_metric=maxRecall,obj=logistic_obj)\n",
    "\n",
    "# 预测\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred_binary = [1 if p > 0.1 else 0 for p in y_pred]  # 将概率转换为类别\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
